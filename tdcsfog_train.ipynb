{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45eeca31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T14:43:45.807604Z",
     "iopub.status.busy": "2023-07-14T14:43:45.807209Z",
     "iopub.status.idle": "2023-07-14T14:43:45.811511Z",
     "shell.execute_reply": "2023-07-14T14:43:45.810620Z",
     "shell.execute_reply.started": "2023-07-14T14:43:45.807577Z"
    }
   },
   "outputs": [],
   "source": [
    "# This file is modified version of original:\n",
    "# https://www.kaggle.com/code/baurzhanurazalinov/parkinson-s-freezing-tdcsfog-training-code/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad51dd8d",
   "metadata": {
    "papermill": {
     "duration": 0.00482,
     "end_time": "2023-06-13T03:02:36.847375",
     "exception": false,
     "start_time": "2023-06-13T03:02:36.842555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ba1f071",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T14:43:45.813251Z",
     "iopub.status.busy": "2023-07-14T14:43:45.812990Z",
     "iopub.status.idle": "2023-07-14T14:43:45.826823Z",
     "shell.execute_reply": "2023-07-14T14:43:45.826085Z",
     "shell.execute_reply.started": "2023-07-14T14:43:45.813227Z"
    }
   },
   "outputs": [],
   "source": [
    "# run with different configurations\n",
    "# Model 1 => val_subjects = ['07285e', '220a17', '54ee6e', '312788', '24a59d', '4bb5d0', '48fd62', '79011a', '7688c1']\n",
    "# Model 2 => \n",
    "# val_subjects = ['07285e', '220a17', '54ee6e', '312788', '24a59d', '4bb5d0', '48fd62', '79011a', '7688c1']\n",
    "# CFG = {..., 'fog_model_dim': 256, ..., 'fog_model_num_encoder_layers': 3, ... }\n",
    "# LEARNING_RATE = 0.01/24\n",
    "# GPU_BATCH_SIZE = 16\n",
    "\n",
    "# Model 3 => val_subjects = ['e39bc5', '516a67', 'af82b2', '4dc2f8', '743f4e', 'fa8764', 'a03db7', '51574c', '2d57c2']\n",
    "# Model 4 => val_subjects = ['5c0b8a', 'a03db7', '7fcee9', '2c98f7', '2a39f8', '4f13b4', 'af82b2', 'f686f0', '93f49f', '194d1d', '02bc69', '082f01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb357b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T14:43:45.832409Z",
     "iopub.status.busy": "2023-07-14T14:43:45.832098Z",
     "iopub.status.idle": "2023-07-14T14:43:45.848682Z",
     "shell.execute_reply": "2023-07-14T14:43:45.847935Z",
     "shell.execute_reply.started": "2023-07-14T14:43:45.832383Z"
    },
    "papermill": {
     "duration": 0.029145,
     "end_time": "2023-06-13T03:02:36.881046",
     "exception": false,
     "start_time": "2023-06-13T03:02:36.851901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG = {'TPU': 1, \n",
    "       'block_size': 15552, # must be divisible by block_stride as well as patch_size.\n",
    "       'block_stride': 15552//16,\n",
    "       'patch_size': 18, \n",
    "       \n",
    "       'fog_model_dim': 320,#320 @ except model 2\n",
    "       'fog_model_num_heads': 6,\n",
    "       'fog_model_num_encoder_layers': 5,# 5 @ except model 2\n",
    "       'fog_model_num_lstm_layers': 2,\n",
    "       'fog_model_first_dropout': 0.1,\n",
    "       'fog_model_encoder_dropout': 0.1,\n",
    "       'fog_model_mha_dropout': 0.0,\n",
    "      }\n",
    "\n",
    "LEARNING_RATE = 0.000263158 #0.01/38 @ except model 2\n",
    "STEPS_PER_EPOCH = 64\n",
    "WARMUP_STEPS = 64\n",
    "EPOCHS = 50\n",
    "WEIGHTS = ''\n",
    "\n",
    "# validation subjects.\n",
    "val_subjects = ['07285e', '220a17', '54ee6e', '312788', '24a59d', '4bb5d0', '48fd62', '79011a', '7688c1']\n",
    "\n",
    "assert CFG['block_size'] % CFG['patch_size'] == 0\n",
    "assert CFG['block_size'] % CFG['block_stride'] == 0\n",
    "\n",
    "'''\n",
    "Train and inference batch size\n",
    "\n",
    "'''\n",
    "\n",
    "GPU_BATCH_SIZE = 2#32, 4 @ local\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Mean-std normalization function. \n",
    "Example input: shape (5000), dtype np.float32\n",
    "Example output: shape (5000), dtype np.float32\n",
    "\n",
    "Used to normalize AccV, AccML, AccAP values.\n",
    "\n",
    "'''\n",
    "\n",
    "def sample_normalize(sample):\n",
    "    # tf.math.reduce_mean( input_tensor, axis=None,) => \n",
    "    # reduces input_tensor along the dimensions given in axis by computing the mean of elements across the dimensions in axis.\n",
    "    mean = tf.math.reduce_mean(sample)\n",
    "    std = tf.math.reduce_std(sample)\n",
    "    # tf.math.divide_no_nan(x,y) => computes a safe divide which returns 0 if y (denominator) is zero.\n",
    "    sample = tf.math.divide_no_nan(sample-mean, std)\n",
    "    \n",
    "    return sample.numpy()\n",
    "\n",
    "'''\n",
    "Function for splitting a series into blocks. Blocks can overlap. \n",
    "How the function works:\n",
    "Suppose we have a series with AccV, AccML, AccAP columns and len of 50000, that is (50000, 3). \n",
    "First, the series is padded so that the final length is divisible by CFG['block_size'] = 15552. Now the series shape is (62208, 3).\n",
    "Then we get blocks: first block is series[0:15552, :], second block is series[972:16524, :], ... , last block is series[46656:62208, :].\n",
    "\n",
    "'''\n",
    "\n",
    "def get_blocks(series, columns):\n",
    "    series = series.copy()\n",
    "    series = series[columns]\n",
    "    \n",
    "    # series.head(3) =>\n",
    "    #        AccV     AccML     AccAP  StartHesitation  Turn  Walking  Valid  Mask\n",
    "    # 0 -0.276487 -0.169868 -1.734705                0     0        0      1     1\n",
    "    # 1 -0.278077 -0.171851 -1.746803                0     0        0      1     1\n",
    "    # 2 -0.273169 -0.174003 -1.741763                0     0        0      1     1    \n",
    "    \n",
    "    # series.values =>\n",
    "    # [[-0.27648746 -0.16986818 -1.73470538 ...  0.          1.\n",
    "    #    1.        ]\n",
    "    #  [-0.27807749 -0.17185063 -1.74680347 ...  0.          1.\n",
    "    #    1.        ]\n",
    "    #  ...\n",
    "    #  [-0.1392557  -0.35189566 -2.12723026 ...  0.          1.\n",
    "    #    1.        ]\n",
    "    #  [-0.14250613 -0.33945229 -2.14632764 ...  0.          1.\n",
    "    #    1.        ]]\n",
    "    \n",
    "    # series.values.shape => (4682, 8)\n",
    "    # type(series.values) => <class 'numpy.ndarray'>\n",
    "\n",
    "    series = series.values\n",
    "    series = series.astype(np.float32)\n",
    "    \n",
    "    # .ceil => rounds a number UP to the nearest integer.\n",
    "    block_count = math.ceil(len(series) / CFG['block_size'])\n",
    "    \n",
    "    # [0, ...] => pad axis=0 with 0 values before and ... values after.\n",
    "    # [0, 0] => pad axis=1 with 0 values before and 0 values after.\n",
    "    series = np.pad(series, pad_width=[[0, block_count*CFG['block_size']-len(series)], [0, 0]])\n",
    "    \n",
    "    block_begins = list(range(0, len(series), CFG['block_stride']))\n",
    "    block_begins = [x for x in block_begins if x+CFG['block_size'] <= len(series)]\n",
    "    \n",
    "    blocks = []\n",
    "    for begin in block_begins:\n",
    "        values = series[begin:begin+CFG['block_size']]\n",
    "        blocks.append({'begin': begin,\n",
    "                       'end': begin+CFG['block_size'],\n",
    "                       'values': values})\n",
    "    \n",
    "    return blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81207efe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T14:43:45.850791Z",
     "iopub.status.busy": "2023-07-14T14:43:45.850478Z",
     "iopub.status.idle": "2023-07-14T14:43:45.862088Z",
     "shell.execute_reply": "2023-07-14T14:43:45.861370Z",
     "shell.execute_reply.started": "2023-07-14T14:43:45.850764Z"
    }
   },
   "outputs": [],
   "source": [
    "# tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a22011",
   "metadata": {
    "papermill": {
     "duration": 0.004152,
     "end_time": "2023-06-13T03:02:36.889496",
     "exception": false,
     "start_time": "2023-06-13T03:02:36.885344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Imports and Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c563fe3e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-07-14T14:43:45.964061Z",
     "iopub.status.busy": "2023-07-14T14:43:45.963466Z",
     "iopub.status.idle": "2023-07-14T14:44:24.353982Z",
     "shell.execute_reply": "2023-07-14T14:44:24.352628Z",
     "shell.execute_reply.started": "2023-07-14T14:43:45.964028Z"
    },
    "papermill": {
     "duration": 9.402423,
     "end_time": "2023-06-13T03:02:46.296225",
     "exception": false,
     "start_time": "2023-06-13T03:02:36.893802",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D0714 14:44:12.645699167    3249 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\n",
      "D0714 14:44:12.645725509    3249 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\n",
      "D0714 14:44:12.645728974    3249 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\n",
      "D0714 14:44:12.645731619    3249 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\n",
      "D0714 14:44:12.645734177    3249 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\n",
      "D0714 14:44:12.645736663    3249 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\n",
      "D0714 14:44:12.645739164    3249 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\n",
      "D0714 14:44:12.645741548    3249 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\n",
      "D0714 14:44:12.645743927    3249 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\n",
      "D0714 14:44:12.645746464    3249 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\n",
      "D0714 14:44:12.645748879    3249 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\n",
      "D0714 14:44:12.645751334    3249 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\n",
      "D0714 14:44:12.645753874    3249 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\n",
      "D0714 14:44:12.645756264    3249 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\n",
      "I0714 14:44:12.645964923    3249 ev_epoll1_linux.cc:122]               grpc epoll fd: 60\n",
      "D0714 14:44:12.645977376    3249 ev_posix.cc:144]                      Using polling engine: epoll1\n",
      "D0714 14:44:12.645995183    3249 dns_resolver_ares.cc:822]             Using ares dns resolver\n",
      "D0714 14:44:12.646423405    3249 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\n",
      "D0714 14:44:12.646431358    3249 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\n",
      "D0714 14:44:12.646435090    3249 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\n",
      "D0714 14:44:12.646438284    3249 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\n",
      "D0714 14:44:12.646441661    3249 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\n",
      "D0714 14:44:12.646444911    3249 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\n",
      "D0714 14:44:12.646451532    3249 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\n",
      "D0714 14:44:12.646469367    3249 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\n",
      "D0714 14:44:12.646503146    3249 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\n",
      "D0714 14:44:12.646521166    3249 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\n",
      "D0714 14:44:12.646524788    3249 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\n",
      "D0714 14:44:12.646528161    3249 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\n",
      "D0714 14:44:12.646531695    3249 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\n",
      "D0714 14:44:12.646535131    3249 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\n",
      "D0714 14:44:12.646538776    3249 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\n",
      "D0714 14:44:12.646543076    3249 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\n",
      "I0714 14:44:12.649376507    3249 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\n",
      "I0714 14:44:12.671875671    3249 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\n",
      "E0714 14:44:12.678953883    3249 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2023-07-14T14:44:12.678939057+00:00\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
      "INFO:tensorflow:Initializing the TPU system: local\n",
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# if CFG['TPU']:\n",
    "#     !pip install -q /lib/wheels/tensorflow-2.9.1-cp38-cp38-linux_x86_64.whl\n",
    "#     !pip3 install -q U scikit-learn\n",
    "    \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "if CFG['TPU']:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu='local') \n",
    "    tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    TPU_BATCH_SIZE = GPU_BATCH_SIZE = tpu_strategy.num_replicas_in_sync*GPU_BATCH_SIZE\n",
    "    \n",
    "    \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def folder(path): \n",
    "    if not os.path.exists(path): os.makedirs(path)\n",
    "        \n",
    "def plot(e, size=(20, 4)):\n",
    "    plt.figure(figsize=size)\n",
    "    plt.plot(e)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f5a94",
   "metadata": {
    "papermill": {
     "duration": 0.003974,
     "end_time": "2023-06-13T03:02:46.304584",
     "exception": false,
     "start_time": "2023-06-13T03:02:46.300610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a0116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c86a8e63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T14:44:24.356945Z",
     "iopub.status.busy": "2023-07-14T14:44:24.356417Z",
     "iopub.status.idle": "2023-07-14T14:44:24.390777Z",
     "shell.execute_reply": "2023-07-14T14:44:24.389976Z",
     "shell.execute_reply.started": "2023-07-14T14:44:24.356904Z"
    },
    "papermill": {
     "duration": 0.028218,
     "end_time": "2023-06-13T03:02:46.336980",
     "exception": false,
     "start_time": "2023-06-13T03:02:46.308762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The transformer encoder layer\n",
    "For more details, see https://arxiv.org/pdf/1706.03762.pdf [Attention Is All You Need]\n",
    "\n",
    "'''\n",
    "\n",
    "class EncoderLayer(tf.keras.Model):#tf.keras.Model tf.keras.layers.Layer # comment by me\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=CFG['fog_model_num_heads'], key_dim=CFG['fog_model_dim'], dropout=CFG['fog_model_mha_dropout'])\n",
    "        \n",
    "        self.add = tf.keras.layers.Add()        \n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "        self.seq = tf.keras.Sequential([tf.keras.layers.Dense(CFG['fog_model_dim'], activation='relu'), \n",
    "                                        tf.keras.layers.Dropout(CFG['fog_model_encoder_dropout']), \n",
    "                                        tf.keras.layers.Dense(CFG['fog_model_dim']), \n",
    "                                        tf.keras.layers.Dropout(CFG['fog_model_encoder_dropout']),\n",
    "                                       ])\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        attn_output = self.mha(query=x, key=x, value=x)\n",
    "        # attn_output.shape => (2, 864, 320)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layernorm(x)\n",
    "        # x.shape => (2, 864, 320)        \n",
    "        return x\n",
    "    \n",
    "    def model(self):\n",
    "        # input_shape=(not include batch size).\n",
    "        x = tf.keras.Input(shape=( 864, 320), \n",
    "                                       batch_size=GPU_BATCH_SIZE)\n",
    "        return tf.keras.Model(inputs=x, outputs=self.call(x))    \n",
    "    \n",
    "'''\n",
    "FOGEncoder is a combination of transformer encoder (D=320, H=6, L=5) and two BidirectionalLSTM layers\n",
    "\n",
    "'''\n",
    "\n",
    "class FOGEncoder(tf.keras.Model):#tf.keras.Model tf.keras.layers.Layer # comment by me\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.first_linear = tf.keras.layers.Dense(CFG['fog_model_dim'])        \n",
    "        \n",
    "        self.sequence_len = CFG['block_size'] // CFG['patch_size']\n",
    "        self.pos_encoding = tf.Variable(initial_value=tf.random.normal(shape=(1, self.sequence_len, CFG['fog_model_dim']), stddev=0.02), trainable=True) \n",
    "        \n",
    "        self.add = tf.keras.layers.Add()\n",
    "        \n",
    "        self.first_dropout = tf.keras.layers.Dropout(CFG['fog_model_first_dropout'])\n",
    "        \n",
    "        self.enc_layers = tf.keras.Sequential([EncoderLayer() for _ in range(CFG['fog_model_num_encoder_layers'])])\n",
    "        \n",
    "        self.lstm_layers = tf.keras.Sequential([tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(CFG['fog_model_dim'], return_sequences=True)) for _ in range(CFG['fog_model_num_lstm_layers'])])\n",
    "\n",
    "        \n",
    "    def call(self, x, training=None): \n",
    "        x = x / 25.0 # Normalization attempt in the segment [-1, 1]\n",
    "        x = self.first_linear(x) \n",
    "          \n",
    "        if training: # augmentation by randomly roll of the position encoding tensor\n",
    "            # tf.tile => \n",
    "            # output tensor's 0'th dimension has  self.pos_encoding[0]* GPU_BATCH_SIZE elements, -\n",
    "            # - and the values of input are kept GPU_BATCH_SIZE times along the '0'th dimension.\n",
    "            # output tensor's 1'th dimension has self.pos_encoding[1] * 1 elements, -\n",
    "            # - and the values of input are kept 1 times along the '1'th dimension.            \n",
    "            # output tensor's 2'th dimension has self.pos_encoding[2] * 1 elements, -\n",
    "            # - and the values of input are kept 1 times along the '2'th dimension.  \n",
    "            \n",
    "            # tf.roll => \n",
    "            # The elements are shifted positively (towards larger indices) by the offset of shift along the dimension of axis. -\n",
    "            # - Negative shift values will shift elements in the opposite direction. \n",
    "            # tf.random.uniform(shape=(2,), minval=-864, maxval=0, dtype=tf.int32).numpy() => array([-838, -624], dtype=int32)\n",
    "            random_pos_encoding = tf.roll(tf.tile(self.pos_encoding, multiples=[GPU_BATCH_SIZE, 1, 1]), \n",
    "                                          shift=tf.random.uniform(shape=(GPU_BATCH_SIZE,), minval=-self.sequence_len, maxval=0, dtype=tf.int32),\n",
    "                                          axis=GPU_BATCH_SIZE * [1],# 2 * [1] => [1, 1]\n",
    "                                          )\n",
    "            x = self.add([x, random_pos_encoding])\n",
    "        \n",
    "        else: # without augmentation \n",
    "            # tf.tile(self.pos_encoding, multiples=[GPU_BATCH_SIZE, 1, 1]).shape => (GPU_BATCH_SIZE, 864, 320)\n",
    "            x = self.add([x, tf.tile(self.pos_encoding, multiples=[GPU_BATCH_SIZE, 1, 1])])\n",
    "            \n",
    "        x = self.first_dropout(x)\n",
    "        \n",
    "        #for i in range(CFG['fog_model_num_encoder_layers']): x = self.enc_layers[i](x)            \n",
    "        #for i in range(CFG['fog_model_num_lstm_layers']): x = self.lstm_layers[i](x) \n",
    "        x = self.enc_layers(x)                        \n",
    "        x = self.lstm_layers(x)         \n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def model(self):\n",
    "        x = tf.keras.Input(shape=( 864, 54), \n",
    "                                       batch_size=GPU_BATCH_SIZE)\n",
    "        return tf.keras.Model(inputs=x, outputs=self.call(x))    \n",
    "    \n",
    "class FOGModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.encoder = FOGEncoder()\n",
    "        self.last_linear = tf.keras.layers.Dense(3) \n",
    "        \n",
    "    def call(self, x): \n",
    "        x = self.encoder(x)                 \n",
    "        x = self.last_linear(x) \n",
    "        x = tf.nn.sigmoid(x) \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def model(self):\n",
    "        # input_shape=(not include batch size).\n",
    "        x = tf.keras.Input(shape=( CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3), \n",
    "                                       batch_size=GPU_BATCH_SIZE)\n",
    "        return tf.keras.Model(inputs=x, outputs=self.call(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c037ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c451526f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T14:44:24.392512Z",
     "iopub.status.busy": "2023-07-14T14:44:24.392082Z",
     "iopub.status.idle": "2023-07-14T14:44:24.405646Z",
     "shell.execute_reply": "2023-07-14T14:44:24.404876Z",
     "shell.execute_reply.started": "2023-07-14T14:44:24.392475Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = FOGModel().model()\n",
    "# model.compile(loss=loss_function, optimizer=tf.keras.optimizers.Adam(learning_rate=CustomSchedule(LEARNING_RATE, WARMUP_STEPS), beta_1=0.9, beta_2=0.98, epsilon=1e-9))\n",
    "# model.fit(dataset, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, callbacks=[PredictionFnCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0995cd6d",
   "metadata": {
    "papermill": {
     "duration": 0.003967,
     "end_time": "2023-06-13T03:02:46.345108",
     "exception": false,
     "start_time": "2023-06-13T03:02:46.341141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Tdcsfog preparing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2690f467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1971ad0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5d6bfc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T14:44:24.408761Z",
     "iopub.status.busy": "2023-07-14T14:44:24.408403Z",
     "iopub.status.idle": "2023-07-14T14:44:46.590189Z",
     "shell.execute_reply": "2023-07-14T14:44:46.588895Z",
     "shell.execute_reply.started": "2023-07-14T14:44:24.408727Z"
    },
    "papermill": {
     "duration": 34.250404,
     "end_time": "2023-06-13T03:03:20.599674",
     "exception": false,
     "start_time": "2023-06-13T03:02:46.349270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 100%|██████████| 833/833 [00:22<00:00, 37.62it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create train blocks with AccV, AccML, AccAP, StartHesitation, Turn, Walking, Valid, Mask columns and save in the directory\n",
    "\n",
    "'''\n",
    "\n",
    "# pd.read_csv('tdcsfog_metadata.csv').head(3) =>\n",
    "#            Id Subject  Visit  Test Medication\n",
    "# 0  003f117e14  4dc2f8      3     2         on\n",
    "# 1  009ee11563  f62eec      4     2         on\n",
    "# 2  011322847a  231c3b      2     2         on\n",
    "\n",
    "# save_path = './kaggle/working/train/tdcsfog'; folder(save_path); \n",
    "# tdcsfog_metadata = pd.read_csv('tdcsfog_metadata.csv').set_index('Id')\n",
    "# fetch over kaggle:\n",
    "save_path = '/kaggle/working/train/tdcsfog'; folder(save_path); \n",
    "tdcsfog_metadata = pd.read_csv('/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/tdcsfog_metadata.csv').set_index('Id')\n",
    "\n",
    "# tdcsfog_metadata.head(3) =>\n",
    "#            Subject  Visit  Test Medication\n",
    "# Id                                        \n",
    "# 003f117e14  4dc2f8      3     2         on\n",
    "# 009ee11563  f62eec      4     2         on\n",
    "# 011322847a  231c3b      2     2         on\n",
    "\n",
    "blocks_descriptions = []\n",
    "# total=number of expected iterations.\n",
    "for Id in tqdm(tdcsfog_metadata.index, total=len(tdcsfog_metadata.index), desc='Preparing'):\n",
    "    #series = pd.read_csv(f'train/tdcsfog/{Id}.csv')\n",
    "    series = pd.read_csv(f'/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog/{Id}.csv')\n",
    "    \n",
    "    # series.head(3) =>\n",
    "    #    Time      AccV     AccML     AccAP  StartHesitation  Turn  Walking\n",
    "    # 0     0 -9.533939  0.566322 -1.413525                0     0        0\n",
    "    # 1     1 -9.536140  0.564137 -1.440621                0     0        0\n",
    "    # 2     2 -9.529345  0.561765 -1.429332                0     0        0    \n",
    "    \n",
    "    # series['AccV'].values => [-9.5339393  -9.5361403   ... -9.3439773 -9.34847668]\n",
    "    # type(series['AccV'].values) => <class 'numpy.ndarray'>\n",
    "    # series['AccV'].values.shape => (4682,)\n",
    "    \n",
    "    series['AccV'] = sample_normalize(series['AccV'].values)\n",
    "    series['AccML'] = sample_normalize(series['AccML'].values)\n",
    "    series['AccAP'] = sample_normalize(series['AccAP'].values)\n",
    "    series['Valid'] = 1\n",
    "    series['Mask'] = 1\n",
    "    \n",
    "    blocks = get_blocks(series, ['AccV', 'AccML', 'AccAP', 'StartHesitation', 'Turn', 'Walking', 'Valid', 'Mask'])\n",
    "    # break # my break\n",
    "\n",
    "    for block_count, block in enumerate(blocks):\n",
    "        fname, values = f'{Id}_{block_count}.npy', block['values']\n",
    "        block_description = {}\n",
    "        block_description['Id'] = Id\n",
    "        block_description['Count'] = block_count\n",
    "        block_description['File'] = fname\n",
    "        block_description['Path'] = f'{save_path}/{fname}'\n",
    "        block_description['Source'] = 'tsfog'\n",
    "        \n",
    "        block_description['StartHesitation_size'] = np.sum(values[:, 3])\n",
    "        block_description['Turn_size'] = np.sum(values[:, 4])\n",
    "        block_description['Walking_size'] = np.sum(values[:, 5])\n",
    "        block_description['Valid_size'] = np.sum(values[:, 6])\n",
    "        block_description['Mask_size'] = np.sum(values[:, 7])\n",
    "\n",
    "        blocks_descriptions.append(block_description)\n",
    "        np.save(f'{save_path}/{fname}', values)\n",
    "\n",
    "blocks_descriptions = pd.DataFrame(blocks_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97e7412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b3ce1ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T14:44:46.591806Z",
     "iopub.status.busy": "2023-07-14T14:44:46.591455Z",
     "iopub.status.idle": "2023-07-14T14:44:46.616725Z",
     "shell.execute_reply": "2023-07-14T14:44:46.615320Z",
     "shell.execute_reply.started": "2023-07-14T14:44:46.591775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Count</th>\n",
       "      <th>File</th>\n",
       "      <th>Path</th>\n",
       "      <th>Source</th>\n",
       "      <th>StartHesitation_size</th>\n",
       "      <th>Turn_size</th>\n",
       "      <th>Walking_size</th>\n",
       "      <th>Valid_size</th>\n",
       "      <th>Mask_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003f117e14</td>\n",
       "      <td>0</td>\n",
       "      <td>003f117e14_0.npy</td>\n",
       "      <td>/kaggle/working/train/tdcsfog/003f117e14_0.npy</td>\n",
       "      <td>tsfog</td>\n",
       "      <td>0.0</td>\n",
       "      <td>788.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4682.0</td>\n",
       "      <td>4682.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>009ee11563</td>\n",
       "      <td>0</td>\n",
       "      <td>009ee11563_0.npy</td>\n",
       "      <td>/kaggle/working/train/tdcsfog/009ee11563_0.npy</td>\n",
       "      <td>tsfog</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4341.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9920.0</td>\n",
       "      <td>9920.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>011322847a</td>\n",
       "      <td>0</td>\n",
       "      <td>011322847a_0.npy</td>\n",
       "      <td>/kaggle/working/train/tdcsfog/011322847a_0.npy</td>\n",
       "      <td>tsfog</td>\n",
       "      <td>0.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5187.0</td>\n",
       "      <td>5187.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id  Count              File   \n",
       "0  003f117e14      0  003f117e14_0.npy  \\\n",
       "1  009ee11563      0  009ee11563_0.npy   \n",
       "2  011322847a      0  011322847a_0.npy   \n",
       "\n",
       "                                             Path Source   \n",
       "0  /kaggle/working/train/tdcsfog/003f117e14_0.npy  tsfog  \\\n",
       "1  /kaggle/working/train/tdcsfog/009ee11563_0.npy  tsfog   \n",
       "2  /kaggle/working/train/tdcsfog/011322847a_0.npy  tsfog   \n",
       "\n",
       "   StartHesitation_size  Turn_size  Walking_size  Valid_size  Mask_size  \n",
       "0                   0.0      788.0           0.0      4682.0     4682.0  \n",
       "1                   0.0     4341.0           0.0      9920.0     9920.0  \n",
       "2                   0.0      281.0           0.0      5187.0     5187.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks_descriptions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39823798",
   "metadata": {
    "papermill": {
     "duration": 0.018247,
     "end_time": "2023-06-13T03:03:20.636481",
     "exception": false,
     "start_time": "2023-06-13T03:03:20.618234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Train Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12e32330",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T14:44:46.618540Z",
     "iopub.status.busy": "2023-07-14T14:44:46.618083Z",
     "iopub.status.idle": "2023-07-14T14:44:53.298728Z",
     "shell.execute_reply": "2023-07-14T14:44:53.297373Z",
     "shell.execute_reply.started": "2023-07-14T14:44:46.618507Z"
    },
    "papermill": {
     "duration": 5.544187,
     "end_time": "2023-06-13T03:03:26.198708",
     "exception": false,
     "start_time": "2023-06-13T03:03:20.654521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Write: 100%|██████████| 2109/2109 [00:06<00:00, 318.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train ids] 717 [Val ids] 116 (13.9)\n",
      "[Train blocks] 2109\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Selecting validation subjects\n",
    "FOGModel train data preparing\n",
    "\n",
    "'''\n",
    "\n",
    "def write_to_ram(fog):\n",
    "    fog = fog[['Id', 'Count', 'Path']]\n",
    "    \n",
    "    for _, row in tqdm(fog.iterrows(), total=len(fog), desc='Write'):\n",
    "        Id, Count, path = row['Id'], row['Count'], row['Path']\n",
    "        \n",
    "        # Read data\n",
    "        series = np.load(path) # ['AccV', 'AccML', 'AccAP', 'StartHesitation', 'Turn', 'Walking', 'Valid', 'Mask']\n",
    "                \n",
    "        # series.shape => (15552, 8)\n",
    "        # Create patches\n",
    "        series = tf.reshape(series, shape=(CFG['block_size'] // CFG['patch_size'], CFG['patch_size'], series.shape[1]))\n",
    "        # series.shape => (864, 18, 8)\n",
    "\n",
    "        # Create input\n",
    "        series_input = series[:, :, 0:3]\n",
    "        # series_input.shape => (864, 18, 3)\n",
    "        series_input = tf.reshape(series_input, shape=(CFG['block_size'] // CFG['patch_size'], -1))\n",
    "        # series_input.shape => (864, 54)\n",
    "\n",
    "        # Create target\n",
    "        series_target = series[:, :, 3:]\n",
    "        # series_target.shape => (864, 18, 5)\n",
    "        # tf.transpose(a, perm=None, ...) => permutes the dimensions according to the value of perm.\n",
    "        series_target = tf.transpose(series_target, perm=[0, 2, 1])\n",
    "        # series_target.shape => (864, 5, 18)\n",
    "        series_target = tf.reduce_max(series_target, axis=-1)\n",
    "        # series_target.shape => (864, 5)\n",
    "        # .cast() => casts a tensor to a new dtype.\n",
    "        series_target = tf.cast(series_target, tf.int32)# tf.int64\n",
    "        # series_target.shape => (864, 5)\n",
    "\n",
    "        # count is the block number for particular Id.\n",
    "        RAM[(Id, Count)] = (series_input, series_target)\n",
    "\n",
    "\n",
    "# tdcsfog_metadata['Subject'].apply(lambda x: x not in val_subjects) =>\n",
    "# Id\n",
    "# 003f117e14     True\n",
    "# 009ee11563     True\n",
    "#               ...  \n",
    "# feba449e1a    False\n",
    "# ffda8fadfd     True\n",
    "# Name: Subject, Length: 833, dtype: bool\n",
    "\n",
    "# tdcsfog_metadata.index => \n",
    "# Index(['003f117e14', '009ee11563', '011322847a', '01d0fe7266', '024418ba39',\n",
    "#        ...\n",
    "#        'fd5300c038', 'fe33f7591d', 'fe4dcf3ded', 'fe7d3b45f2', 'feadfa435d',\n",
    "#       dtype='object', name='Id', length=833)\n",
    "\n",
    "# type(tdcsfog_metadata.index) => <class 'pandas.core.indexes.base.Index'>\n",
    "\n",
    "train_ids = tdcsfog_metadata[tdcsfog_metadata['Subject'].apply(lambda x: x not in val_subjects)].index.tolist()\n",
    "val_ids = tdcsfog_metadata[tdcsfog_metadata['Subject'].apply(lambda x: x in val_subjects)].index.tolist()\n",
    "\n",
    "train_blocks_descriptions = blocks_descriptions[blocks_descriptions['Id'].apply(lambda x: x in train_ids)]\n",
    "\n",
    "RAM = {} \n",
    "write_to_ram(train_blocks_descriptions)\n",
    "\n",
    "print(f'\\n[Train ids] {len(train_ids)} [Val ids] {len(val_ids)} ({100*len(val_ids)/(len(train_ids)+len(val_ids)):.1f})')\n",
    "print(f'[Train blocks] {len(train_blocks_descriptions )}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4600f2a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T14:44:53.300912Z",
     "iopub.status.busy": "2023-07-14T14:44:53.300569Z",
     "iopub.status.idle": "2023-07-14T14:44:56.464183Z",
     "shell.execute_reply": "2023-07-14T14:44:56.462788Z",
     "shell.execute_reply.started": "2023-07-14T14:44:53.300883Z"
    },
    "papermill": {
     "duration": 3.303548,
     "end_time": "2023-06-13T03:03:29.524798",
     "exception": false,
     "start_time": "2023-06-13T03:03:26.221250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create a random train dataset from train_blocks_descriptions DataFrame\n",
    "\n",
    "'''\n",
    "\n",
    "def read(row):\n",
    "    \n",
    "    def read_from_ram(Id, Count):  \n",
    "        series_inputs, series_targets = RAM[(Id.numpy().decode('utf-8'), Count.numpy())]\n",
    "        series_targets = series_targets.numpy().astype(np.float32)\n",
    "        \n",
    "        return series_inputs, series_targets\n",
    "    \n",
    "    # .py_function() => wraps a python function into a TensorFlow op that executes it eagerly. -\n",
    "    # - this function allows expressing computations in a TensorFlow graph as Python functions.\n",
    "    [series_input, series_target] = tf.py_function(read_from_ram, [row['Id'], row['Count']], [tf.float32, tf.float32])\n",
    "    # series_input.shape, series_target.shape => (864, 54), (864, 5)\n",
    "    series_input.set_shape(shape=(CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3))\n",
    "    series_target.set_shape(shape=(CFG['block_size'] // CFG['patch_size'], 5))\n",
    "    # series_input.shape, series_target.shape => (864, 54), (864, 5)\n",
    "    \n",
    "    return series_input, series_target\n",
    "\n",
    "  \n",
    "groups = [group.aggregate(dict, axis=1).tolist() for Id, group in train_blocks_descriptions.groupby('Id')]\n",
    "# Id => ff4f844fd3\n",
    "\n",
    "# type(group) => <class 'pandas.core.frame.DataFrame'>\n",
    "# group => \n",
    "#              Id  Count               File   ...   Mask_size\n",
    "# 2365  ff4f844fd3      0   ff4f844fd3_0.npy  ...   15552.0\n",
    "# 2366  ff4f844fd3      1   ff4f844fd3_1.npy  ...   14989.0\n",
    "# 2367  ff4f844fd3      2   ff4f844fd3_2.npy  ...   14017.0\n",
    "\n",
    "# type(group.aggregate(dict, axis=1)) => <class 'pandas.core.series.Series'>\n",
    "# .aggregate(func=None, axis=0, ...) => aggregate over the specified axis.\n",
    "# group.aggregate(dict, axis=1) =>\n",
    "# 2365 {'Id': 'ff4f844fd3', 'Count': 0, 'File': 'ff4f844fd3_0.npy', ... 'Mask_size': 15552.0}\n",
    "# 2366 {'Id': 'ff4f844fd3', 'Count': 1, 'File': 'ff4f844fd3_1.npy', ... 'Mask_size': 14989.0}\n",
    "# ...\n",
    "# 2381 {'Id': 'ff4f844fd3', 'Count': 16, 'File': 'ff4f844fd3_16.npy', ... 'Mask_size': 409.0}]\n",
    "# dtype: object\n",
    "\n",
    "# group.aggregate(dict, axis=1).tolist() =>\n",
    "# [{'Id': 'ff4f844fd3', 'Count': 0, 'File': 'ff4f844fd3_0.npy', ... 'Mask_size': 15552.0}, \n",
    "# {'Id': 'ff4f844fd3', 'Count': 1, 'File': 'ff4f844fd3_1.npy', ... 'Mask_size': 14989.0},\n",
    "# ...\n",
    "# {'Id': 'ff4f844fd3', 'Count': 16, 'File': 'ff4f844fd3_16.npy', ... 'Mask_size': 409.0}]\n",
    "\n",
    "\n",
    "random.shuffle(groups)\n",
    "# itertools.cycle(iterable) => \n",
    "# make an iterator returning elements from the iterable and saving a copy of each. -\n",
    "# - when the iterable is exhausted, return elements from the saved copy. repeats indefinitely.\n",
    "groups = cycle(groups)\n",
    "\n",
    "dataset, iterator = [], 0\n",
    "while len(dataset) <= 500000:\n",
    "    # next(iterator, default) => retrieve the next item from the iterator.  \n",
    "    group = next(groups)    \n",
    "    # pick random sample from list of dictionaries of a particular Id.\n",
    "    sample = random.choice(group)\n",
    "    # sample => {'Id': '267f36cd04', 'Count': 0, 'File': '267f36cd04_0.npy', ..., 'Mask_size': 4239.0}\n",
    "    dataset.append(sample)\n",
    "    iterator += 1\n",
    "\n",
    "\n",
    "# pd.DataFrame(dataset).head(3) =>\n",
    "#            Id  Count              File  ...  Mask_size\n",
    "# 0  4025712647      0  4025712647_0.npy  ...  3943.0\n",
    "# 1  6d9b1fc826      0  6d9b1fc826_0.npy  ...  7536.0\n",
    "# 2  36f2e89275      0  36f2e89275_0.npy  ...  5497.0 \n",
    "\n",
    "# dict(pd.DataFrame(dataset)) =>\n",
    "# {'Id': 0         4025712647\n",
    "# 1         6d9b1fc826\n",
    "#              ...    \n",
    "# 500000    7d42d53fdd\n",
    "# Name: Id, Length: 500001, dtype: object, \n",
    "# 'Count': 0         0\n",
    "# 1         0\n",
    "#          ...\n",
    "# 500000    0\n",
    "# Name: Count, Length: 500001, dtype: int64, \n",
    "# 'File': 0         4025712647_0.npy\n",
    "# 1         6d9b1fc826_0.npy\n",
    "#                 ...       \n",
    "# 500000    7d42d53fdd_0.npy\n",
    "# Name: File, Length: 500001, dtype: object, \n",
    "# ...,\n",
    "# 'Mask_size': 0          3943.0\n",
    "# 1          7536.0\n",
    "#            ...   \n",
    "# 500000    14337.0\n",
    "# Name: Mask_size, Length: 500001, dtype: float64}\n",
    "\n",
    "# len(dataset) => 500001\n",
    "# .from_tensor_slices(..) =>  creates a Dataset whose elements are slices of the given tensors.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(dict(pd.DataFrame(dataset)))\n",
    "# dataset.map() => Maps map_func across the elements of the dataset.\n",
    "# under tpu_strategy.scope(), we have to adjust batch_size with replicas.\n",
    "dataset = dataset.map(read).batch(TPU_BATCH_SIZE*tpu_strategy.num_replicas_in_sync if CFG['TPU'] else GPU_BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d6b3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "521d0c3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T14:44:56.465734Z",
     "iopub.status.busy": "2023-07-14T14:44:56.465390Z",
     "iopub.status.idle": "2023-07-14T14:44:56.470609Z",
     "shell.execute_reply": "2023-07-14T14:44:56.469660Z",
     "shell.execute_reply.started": "2023-07-14T14:44:56.465705Z"
    }
   },
   "outputs": [],
   "source": [
    "# for element in dataset:    \n",
    "#     # element['Id'] => tf.Tensor(b'267f36cd04', shape=(), dtype=string)\n",
    "#     if element['Id'].numpy().decode(\"utf-8\")=='ff4f844fd3': \n",
    "#         print(element)\n",
    "#         read(element)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0470e465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f8347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59d8ff39",
   "metadata": {
    "papermill": {
     "duration": 0.021969,
     "end_time": "2023-06-13T03:03:29.569296",
     "exception": false,
     "start_time": "2023-06-13T03:03:29.547327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "770815ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T14:49:13.652291Z",
     "iopub.status.busy": "2023-07-14T14:49:13.651375Z",
     "iopub.status.idle": "2023-07-14T14:58:52.620666Z",
     "shell.execute_reply": "2023-07-14T14:58:52.618823Z",
     "shell.execute_reply.started": "2023-07-14T14:49:13.652252Z"
    },
    "papermill": {
     "duration": 216.819576,
     "end_time": "2023-06-13T03:07:06.410804",
     "exception": false,
     "start_time": "2023-06-13T03:03:29.591228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fog_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " fog_encoder_1 (FOGEncoder)  multiple                  17744000  \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| dense_12 (Dense)          multiple                  17600     |\n",
      "|                                                               |\n",
      "| add_6 (Add)               multiple                  0         |\n",
      "|                                                               |\n",
      "| dropout_11 (Dropout)      multiple                  0         |\n",
      "|                                                               |\n",
      "| sequential_12 (Sequential)  (16, 864, 320)          13348800  |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| encoder_layer_5 (EncoderLay  (16, 864, 320)       2669760   ||\n",
      "|| er)                                                         ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| multi_head_attention_5 (Mul  multiple           2463680   |||\n",
      "||| tiHeadAttention)                                          |||\n",
      "|||                                                           |||\n",
      "||| add_7 (Add)           multiple                  0         |||\n",
      "|||                                                           |||\n",
      "||| layer_normalization_5 (Laye  multiple           640       |||\n",
      "||| rNormalization)                                           |||\n",
      "|||                                                           |||\n",
      "||| sequential_7 (Sequential)  (16, 864, 320)       205440    |||\n",
      "||||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||||\n",
      "|||| dense_13 (Dense)    (16, 864, 320)            102720    ||||\n",
      "||||                                                         ||||\n",
      "|||| dropout_12 (Dropout)  (16, 864, 320)          0         ||||\n",
      "||||                                                         ||||\n",
      "|||| dense_14 (Dense)    (16, 864, 320)            102720    ||||\n",
      "||||                                                         ||||\n",
      "|||| dropout_13 (Dropout)  (16, 864, 320)          0         ||||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| encoder_layer_6 (EncoderLay  (16, 864, 320)       2669760   ||\n",
      "|| er)                                                         ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| multi_head_attention_6 (Mul  multiple           2463680   |||\n",
      "||| tiHeadAttention)                                          |||\n",
      "|||                                                           |||\n",
      "||| add_8 (Add)           multiple                  0         |||\n",
      "|||                                                           |||\n",
      "||| layer_normalization_6 (Laye  multiple           640       |||\n",
      "||| rNormalization)                                           |||\n",
      "|||                                                           |||\n",
      "||| sequential_8 (Sequential)  (16, 864, 320)       205440    |||\n",
      "||||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||||\n",
      "|||| dense_15 (Dense)    (16, 864, 320)            102720    ||||\n",
      "||||                                                         ||||\n",
      "|||| dropout_14 (Dropout)  (16, 864, 320)          0         ||||\n",
      "||||                                                         ||||\n",
      "|||| dense_16 (Dense)    (16, 864, 320)            102720    ||||\n",
      "||||                                                         ||||\n",
      "|||| dropout_15 (Dropout)  (16, 864, 320)          0         ||||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| encoder_layer_7 (EncoderLay  (16, 864, 320)       2669760   ||\n",
      "|| er)                                                         ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| multi_head_attention_7 (Mul  multiple           2463680   |||\n",
      "||| tiHeadAttention)                                          |||\n",
      "|||                                                           |||\n",
      "||| add_9 (Add)           multiple                  0         |||\n",
      "|||                                                           |||\n",
      "||| layer_normalization_7 (Laye  multiple           640       |||\n",
      "||| rNormalization)                                           |||\n",
      "|||                                                           |||\n",
      "||| sequential_9 (Sequential)  (16, 864, 320)       205440    |||\n",
      "||||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||||\n",
      "|||| dense_17 (Dense)    (16, 864, 320)            102720    ||||\n",
      "||||                                                         ||||\n",
      "|||| dropout_16 (Dropout)  (16, 864, 320)          0         ||||\n",
      "||||                                                         ||||\n",
      "|||| dense_18 (Dense)    (16, 864, 320)            102720    ||||\n",
      "||||                                                         ||||\n",
      "|||| dropout_17 (Dropout)  (16, 864, 320)          0         ||||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| encoder_layer_8 (EncoderLay  (16, 864, 320)       2669760   ||\n",
      "|| er)                                                         ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| multi_head_attention_8 (Mul  multiple           2463680   |||\n",
      "||| tiHeadAttention)                                          |||\n",
      "|||                                                           |||\n",
      "||| add_10 (Add)          multiple                  0         |||\n",
      "|||                                                           |||\n",
      "||| layer_normalization_8 (Laye  multiple           640       |||\n",
      "||| rNormalization)                                           |||\n",
      "|||                                                           |||\n",
      "||| sequential_10 (Sequential)  (16, 864, 320)      205440    |||\n",
      "||||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||||\n",
      "|||| dense_19 (Dense)    (16, 864, 320)            102720    ||||\n",
      "||||                                                         ||||\n",
      "|||| dropout_18 (Dropout)  (16, 864, 320)          0         ||||\n",
      "||||                                                         ||||\n",
      "|||| dense_20 (Dense)    (16, 864, 320)            102720    ||||\n",
      "||||                                                         ||||\n",
      "|||| dropout_19 (Dropout)  (16, 864, 320)          0         ||||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| encoder_layer_9 (EncoderLay  (16, 864, 320)       2669760   ||\n",
      "|| er)                                                         ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| multi_head_attention_9 (Mul  multiple           2463680   |||\n",
      "||| tiHeadAttention)                                          |||\n",
      "|||                                                           |||\n",
      "||| add_11 (Add)          multiple                  0         |||\n",
      "|||                                                           |||\n",
      "||| layer_normalization_9 (Laye  multiple           640       |||\n",
      "||| rNormalization)                                           |||\n",
      "|||                                                           |||\n",
      "||| sequential_11 (Sequential)  (16, 864, 320)      205440    |||\n",
      "||||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||||\n",
      "|||| dense_21 (Dense)    (16, 864, 320)            102720    ||||\n",
      "||||                                                         ||||\n",
      "|||| dropout_20 (Dropout)  (16, 864, 320)          0         ||||\n",
      "||||                                                         ||||\n",
      "|||| dense_22 (Dense)    (16, 864, 320)            102720    ||||\n",
      "||||                                                         ||||\n",
      "|||| dropout_21 (Dropout)  (16, 864, 320)          0         ||||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sequential_13 (Sequential)  (16, 864, 640)          4101120   |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| bidirectional_2 (Bidirectio  (16, 864, 640)       1640960   ||\n",
      "|| nal)                                                        ||\n",
      "||                                                             ||\n",
      "|| bidirectional_3 (Bidirectio  (16, 864, 640)       2460160   ||\n",
      "|| nal)                                                        ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " dense_23 (Dense)            multiple                  1923      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,745,923\n",
      "Trainable params: 17,745,923\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "[EventPredictionFnCallback Initialization] [Series] 116 [Blocks] 356\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 14:49:37.413378: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2023-07-14 14:49:37.790369: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - ETA: 0s - loss: 0.2362"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 14:50:25.039426: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2023-07-14 14:50:25.150528: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[0] StartHesitation mAP - 0.024, Turn mAP - 0.514, Walking mAP - 0.085, mAP - 0.208\n",
      "[1] Event mAP - 0.572\n",
      "\n",
      "64/64 [==============================] - 83s 665ms/step - loss: 0.2362\n",
      "Epoch 2/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.1406\n",
      "\n",
      "[0] StartHesitation mAP - 0.353, Turn mAP - 0.752, Walking mAP - 0.116, mAP - 0.407\n",
      "[1] Event mAP - 0.868\n",
      "\n",
      "64/64 [==============================] - 22s 349ms/step - loss: 0.1406\n",
      "Epoch 3/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.1063\n",
      "\n",
      "[0] StartHesitation mAP - 0.431, Turn mAP - 0.820, Walking mAP - 0.080, mAP - 0.444\n",
      "[1] Event mAP - 0.904\n",
      "\n",
      "64/64 [==============================] - 22s 347ms/step - loss: 0.1063\n",
      "Epoch 4/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.1004\n",
      "\n",
      "[0] StartHesitation mAP - 0.047, Turn mAP - 0.782, Walking mAP - 0.076, mAP - 0.302\n",
      "[1] Event mAP - 0.886\n",
      "\n",
      "64/64 [==============================] - 22s 348ms/step - loss: 0.1004\n",
      "Epoch 5/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0912\n",
      "\n",
      "[0] StartHesitation mAP - 0.141, Turn mAP - 0.784, Walking mAP - 0.103, mAP - 0.343\n",
      "[1] Event mAP - 0.904\n",
      "\n",
      "64/64 [==============================] - 22s 348ms/step - loss: 0.0912\n",
      "Epoch 6/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0832\n",
      "\n",
      "[0] StartHesitation mAP - 0.131, Turn mAP - 0.836, Walking mAP - 0.143, mAP - 0.370\n",
      "[1] Event mAP - 0.901\n",
      "\n",
      "64/64 [==============================] - 22s 349ms/step - loss: 0.0832\n",
      "Epoch 7/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0760\n",
      "\n",
      "[0] StartHesitation mAP - 0.150, Turn mAP - 0.806, Walking mAP - 0.125, mAP - 0.360\n",
      "[1] Event mAP - 0.912\n",
      "\n",
      "64/64 [==============================] - 22s 350ms/step - loss: 0.0760\n",
      "Epoch 8/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0723\n",
      "\n",
      "[0] StartHesitation mAP - 0.285, Turn mAP - 0.844, Walking mAP - 0.161, mAP - 0.430\n",
      "[1] Event mAP - 0.895\n",
      "\n",
      "64/64 [==============================] - 23s 356ms/step - loss: 0.0723\n",
      "Epoch 9/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0670\n",
      "\n",
      "[0] StartHesitation mAP - 0.264, Turn mAP - 0.808, Walking mAP - 0.169, mAP - 0.414\n",
      "[1] Event mAP - 0.879\n",
      "\n",
      "64/64 [==============================] - 22s 348ms/step - loss: 0.0670\n",
      "Epoch 10/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0642\n",
      "\n",
      "[0] StartHesitation mAP - 0.191, Turn mAP - 0.806, Walking mAP - 0.178, mAP - 0.391\n",
      "[1] Event mAP - 0.910\n",
      "\n",
      "64/64 [==============================] - 22s 348ms/step - loss: 0.0642\n",
      "Epoch 11/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0533\n",
      "\n",
      "[0] StartHesitation mAP - 0.317, Turn mAP - 0.785, Walking mAP - 0.155, mAP - 0.419\n",
      "[1] Event mAP - 0.900\n",
      "\n",
      "64/64 [==============================] - 22s 347ms/step - loss: 0.0533\n",
      "Epoch 12/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0530\n",
      "\n",
      "[0] StartHesitation mAP - 0.094, Turn mAP - 0.646, Walking mAP - 0.185, mAP - 0.308\n",
      "[1] Event mAP - 0.856\n",
      "\n",
      "64/64 [==============================] - 22s 348ms/step - loss: 0.0530\n",
      "Epoch 13/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0428\n",
      "\n",
      "[0] StartHesitation mAP - 0.466, Turn mAP - 0.812, Walking mAP - 0.169, mAP - 0.482\n",
      "[1] Event mAP - 0.921\n",
      "\n",
      "64/64 [==============================] - 22s 347ms/step - loss: 0.0428\n",
      "Epoch 14/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0430\n",
      "\n",
      "[0] StartHesitation mAP - 0.324, Turn mAP - 0.775, Walking mAP - 0.178, mAP - 0.425\n",
      "[1] Event mAP - 0.918\n",
      "\n",
      "64/64 [==============================] - 22s 348ms/step - loss: 0.0430\n",
      "Epoch 15/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0446\n",
      "\n",
      "[0] StartHesitation mAP - 0.147, Turn mAP - 0.759, Walking mAP - 0.107, mAP - 0.338\n",
      "[1] Event mAP - 0.904\n",
      "\n",
      "64/64 [==============================] - 22s 349ms/step - loss: 0.0446\n",
      "Epoch 16/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0410\n",
      "\n",
      "[0] StartHesitation mAP - 0.279, Turn mAP - 0.830, Walking mAP - 0.154, mAP - 0.421\n",
      "[1] Event mAP - 0.899\n",
      "\n",
      "64/64 [==============================] - 22s 348ms/step - loss: 0.0410\n",
      "Epoch 17/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0386\n",
      "\n",
      "[0] StartHesitation mAP - 0.681, Turn mAP - 0.846, Walking mAP - 0.186, mAP - 0.571\n",
      "[1] Event mAP - 0.897\n",
      "\n",
      "64/64 [==============================] - 23s 357ms/step - loss: 0.0386\n",
      "Epoch 18/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0312\n",
      "\n",
      "[0] StartHesitation mAP - 0.182, Turn mAP - 0.779, Walking mAP - 0.143, mAP - 0.368\n",
      "[1] Event mAP - 0.874\n",
      "\n",
      "64/64 [==============================] - 22s 346ms/step - loss: 0.0312\n",
      "Epoch 19/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0295\n",
      "\n",
      "[0] StartHesitation mAP - 0.340, Turn mAP - 0.809, Walking mAP - 0.146, mAP - 0.432\n",
      "[1] Event mAP - 0.893\n",
      "\n",
      "64/64 [==============================] - 22s 348ms/step - loss: 0.0295\n",
      "Epoch 20/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0262\n",
      "\n",
      "[0] StartHesitation mAP - 0.575, Turn mAP - 0.858, Walking mAP - 0.156, mAP - 0.530\n",
      "[1] Event mAP - 0.899\n",
      "\n",
      "64/64 [==============================] - 22s 347ms/step - loss: 0.0262\n",
      "Epoch 21/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0278\n",
      "\n",
      "[0] StartHesitation mAP - 0.333, Turn mAP - 0.816, Walking mAP - 0.189, mAP - 0.446\n",
      "[1] Event mAP - 0.890\n",
      "\n",
      "64/64 [==============================] - 22s 347ms/step - loss: 0.0278\n",
      "Epoch 22/50\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0307\n",
      "\n",
      "[0] StartHesitation mAP - 0.414, Turn mAP - 0.830, Walking mAP - 0.173, mAP - 0.472\n",
      "[1] Event mAP - 0.902\n",
      "\n",
      "64/64 [==============================] - 22s 348ms/step - loss: 0.0307\n",
      "Epoch 23/50\n",
      "57/64 [=========================>....] - ETA: 2s - loss: 0.0157"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 234\u001b[0m\n\u001b[1;32m    232\u001b[0m         model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mloss_function, optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mCustomSchedule(LEARNING_RATE, WARMUP_STEPS), beta_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, beta_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.98\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-9\u001b[39m))\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;66;03m#!rm -r /kaggle/working/*\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEPS_PER_EPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mPredictionFnCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     model \u001b[38;5;241m=\u001b[39m FOGModel()    \n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "loss_function args exp\n",
    "\n",
    "real is a tensor with the shape (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], 5) where the last axis means:\n",
    "0 - StartHesitation \n",
    "1 - Turn\n",
    "2 - Walking\n",
    "3 - Valid\n",
    "4 - Mask\n",
    "\n",
    "output is a tensor with the shape (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], 3) where the last axis means:\n",
    "0 - StartHesitation predicted\n",
    "1 - Turn predicted\n",
    "2 - Walking predicted\n",
    "\n",
    "'''\n",
    "\n",
    "ce = tf.keras.losses.BinaryCrossentropy(reduction='none')\n",
    "\n",
    "def loss_function(real, output, name='loss_function'):\n",
    "    # real.shape, output.shape => (4, 864, 5) (4, 864, 3)\n",
    "    loss = ce(tf.expand_dims(real[:, :, 0:3], axis=-1), tf.expand_dims(output, axis=-1)) # Example shape (32, 864, 3)\n",
    "    \n",
    "    mask = tf.math.multiply(real[:, :, 3], real[:, :, 4]) # Example shape (32, 864)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    mask = tf.expand_dims(mask, axis=-1) # Example shape (32, 864, 1)\n",
    "    \n",
    "    # output tensor's 2'th dimension has 1 * 3 elements, -\n",
    "    # - and the values of input are kept 3 times along the '2'th dimension.    \n",
    "    mask = tf.tile(mask, multiples=[1, 1, 3]) # Example shape (32, 864, 3)\n",
    "    loss *= mask # Example shape (32, 864, 3)\n",
    "    # loss.shape => (4, 864, 3)\n",
    "    \n",
    "    # tf.reduce_sum(loss) => tf.Tensor(1267.8462, shape=(), dtype=float32)\n",
    "    # tf.reduce_sum(loss).numpy() => 1267.8462\n",
    "    # tf.reduce_sum(mask).numpy() => 5994.0\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "'''\n",
    "Simple learning rate schedule with warm up steps\n",
    "\n",
    "'''\n",
    "        \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_lr, warmup_steps=1):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.initial_lr = tf.cast(initial_lr, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        # returns the min of x and y (i.e. x < y ? x : y) element-wise.\n",
    "        return tf.math.minimum(self.initial_lr, self.initial_lr * (step/self.warmup_steps))  \n",
    "    \n",
    "\n",
    "'''\n",
    "PredictionFnCallback is used for:\n",
    "1. Loading validation data\n",
    "2. FOGModel data preparation\n",
    "3. Prediction\n",
    "4. Scoring and save\n",
    "\n",
    "'''\n",
    "\n",
    "class PredictionFnCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, model=None, verbose=0):\n",
    "        \n",
    "        if not model is None: self.model = model\n",
    "        self.verbose = verbose\n",
    "         \n",
    "        def init(Id, path):\n",
    "            series = pd.read_csv(path).reset_index(drop=True)\n",
    "            series['Id'] = Id\n",
    "            series['AccV'] = sample_normalize(series['AccV'].values)\n",
    "            series['AccML'] = sample_normalize(series['AccML'].values)\n",
    "            series['AccAP'] = sample_normalize(series['AccAP'].values)\n",
    "            # series[['StartHesitation', 'Turn', 'Walking']].head(3) => \n",
    "            #                StartHesitation  Turn  Walking\n",
    "            # 0                0              0        0\n",
    "            # 1                0              0        0\n",
    "            # 2                0              0        0\n",
    "            \n",
    "            # aggregate using one or more operations \"over\" the specified axis. axis=1 i.e., check max in every row.\n",
    "            series['Event'] = series[['StartHesitation', 'Turn', 'Walking']].aggregate('max', axis=1)\n",
    "            # series.head(3) =>\n",
    "            #    Time      AccV     AccML     AccAP  StartHesitation  Turn  Walking  Id            Event\n",
    "            # 0     0  0.714180 -0.175363 -2.228391                0     0        0  02edc527c0      0   \n",
    "            # 1     1 -0.821811  0.636446 -2.110201                0     0        0  02edc527c0      0   \n",
    "            # 2     2 -0.171170  0.285309 -1.933505                0     0        0  02edc527c0      0    \n",
    "            \n",
    "            series_blocks=[]\n",
    "            for block in get_blocks(series, ['AccV', 'AccML', 'AccAP']): # Example shape (15552, 3)\n",
    "                values = tf.reshape(block['values'], shape=(CFG['block_size'] // CFG['patch_size'], CFG['patch_size'], 3)) # Example shape (864, 18, 3)\n",
    "                values = tf.reshape(values, shape=(CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3)) # Example shape (864, 54)\n",
    "                values = tf.expand_dims(values, axis=0) # Example shape (1, 864, 54)\n",
    "                \n",
    "                self.blocks.append(values)\n",
    "                series_blocks.append((self.blocks_counter, block['begin'], block['end']))\n",
    "                self.blocks_counter += 1\n",
    "            \n",
    "            description = {}\n",
    "            description['series'] = series\n",
    "            description['series_blocks'] = series_blocks\n",
    "            self.descriptions.append(description)\n",
    "            \n",
    "        self.descriptions = [] # Blocks metadata\n",
    "        self.blocks = [] # Validation data blocks\n",
    "        self.blocks_counter=0 # Blocks counter\n",
    "        \n",
    "        tsfog_ids = val_ids\n",
    "        #tsfog_paths = [f'train/tdcsfog/{tsfog_id}.csv' for tsfog_id in tsfog_ids]\n",
    "        tsfog_paths = [f'/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog/{tsfog_id}.csv' for tsfog_id in tsfog_ids]\n",
    "        \n",
    "        # \"disable\" argument which you can set to True to silence any tqdm output         \n",
    "        for tsfog_id, tsfog_path in tqdm(zip(tsfog_ids, tsfog_paths), total=len(tsfog_ids), desc='PredictionFnCallback Initialization', disable=1-verbose): \n",
    "            init(tsfog_id, tsfog_path)\n",
    "            \n",
    "        self.blocks = tf.concat(self.blocks, axis=0) # Example shape (self.blocks_counter, 864, 54)\n",
    "        \n",
    "        '''\n",
    "        self.blocks is padded so that the final length is divisible by inference batch size for error-free operation of model.predict function\n",
    "        Padded values have no effect on the predictions\n",
    "        \n",
    "        '''\n",
    "        # under tpu_strategy.scope(), we have to adjust batch_size with replicas.\n",
    "        self.blocks = tf.pad(self.blocks, \n",
    "                             paddings=[[0, math.ceil(self.blocks_counter / (TPU_BATCH_SIZE*tpu_strategy.num_replicas_in_sync if CFG['TPU'] else GPU_BATCH_SIZE))*(TPU_BATCH_SIZE*tpu_strategy.num_replicas_in_sync if CFG['TPU'] else GPU_BATCH_SIZE)-self.blocks_counter], \n",
    "                                                    [0, 0], \n",
    "                                                    [0, 0],\n",
    "                                      ]) # Example shape (self.blocks_counter+pad_value, 864, 54)\n",
    "        \n",
    "        print(f'\\n[EventPredictionFnCallback Initialization] [Series] {len(self.descriptions)} [Blocks] {self.blocks_counter}\\n')\n",
    "    \n",
    "    def prediction(self):\n",
    "        # under tpu_strategy.scope(), we have to adjust batch_size with replicas.\n",
    "        predictions = model.predict(self.blocks, batch_size=TPU_BATCH_SIZE*tpu_strategy.num_replicas_in_sync if CFG['TPU'] else GPU_BATCH_SIZE, verbose=self.verbose) # Example shape (self.blocks_counter+pad_value, 864, 3)\n",
    "        predictions = tf.expand_dims(predictions, axis=-1) # Example shape (self.blocks_counter+pad_value, 864, 3, 1)\n",
    "        predictions = tf.transpose(predictions, perm=[0, 1, 3, 2]) # Example shape (self.blocks_counter+pad_value, 864, 1, 3)\n",
    "        \n",
    "        # output tensor's 2'th dimension has 1 * CFG['patch_size'] elements, -\n",
    "        # - and the values of input are kept CFG['patch_size'] times along the '2'th dimension.\n",
    "        predictions = tf.tile(predictions, multiples=[1, 1, CFG['patch_size'], 1]) # Example shape (self.blocks_counter+pad_value, 864, 18, 3)\n",
    "        predictions = tf.reshape(predictions, shape=(predictions.shape[0], predictions.shape[1]*predictions.shape[2], 3)) # Example shape (self.blocks_counter+pad_value, 15552, 3)\n",
    "        predictions = predictions.numpy()\n",
    "        \n",
    "        '''\n",
    "        The following function aggregates predictions blocks and creates dataframes with StartHesitation_prediction, Turn_prediction, Walking_prediction columns.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        def create_target(description):\n",
    "            series, series_blocks = description['series'].copy(), description['series_blocks']\n",
    "\n",
    "            # series_blocks[-1] => (0, 0, 15552)\n",
    "            # series_blocks[-1][2] => 15552 \n",
    "            values = np.zeros((series_blocks[-1][2], 4))\n",
    "            # values.shape => (15552, 4)\n",
    "            # predictions[0].shape => (15552, 3)\n",
    "            # predictions[0] =>\n",
    "            # [[0.43343315 0.4191133  0.4423046 ]\n",
    "            #  ...\n",
    "            #  [0.48576653 0.48780546 0.4736595 ]]            \n",
    "            \n",
    "            for series_block in series_blocks:\n",
    "                i, begin, end = series_block\n",
    "                values[begin:end, 0:3] += predictions[i]\n",
    "                values[begin:end, 3] += 1\n",
    "\n",
    "            values = values[:len(series)]\n",
    "            \n",
    "            series['StartHesitation_prediction'] = values[:, 0] / values[:, 3]\n",
    "            series['Turn_prediction'] = values[:, 1] / values[:, 3]\n",
    "            series['Walking_prediction'] = values[:, 2] / values[:, 3]\n",
    "            series['Prediction_count'] = values[:, 3]\n",
    "            series['Event_prediction'] = series[['StartHesitation_prediction', 'Turn_prediction', 'Walking_prediction']].aggregate('max', axis=1)\n",
    "            \n",
    "            return series\n",
    "        \n",
    "        targets = Parallel(n_jobs=-1, verbose=0\n",
    "                          )(delayed(create_target)(self.descriptions[i]) for i in tqdm(range(len(self.descriptions)), disable=1-self.verbose))        \n",
    "                      \n",
    "        targets = pd.concat(targets)\n",
    "        \n",
    "        return targets\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        scores=[]\n",
    "        scores.append(f'{(epoch+1):03d}')\n",
    "        \n",
    "        loss = logs['loss'] if epoch >= 0 else 1.0\n",
    "        \n",
    "        targets = self.prediction()\n",
    "        \n",
    "        # Score            \n",
    "        StartHesitation_mAP = average_precision_score(targets['StartHesitation'], targets['StartHesitation_prediction'])\n",
    "        Turn_mAP = average_precision_score(targets['Turn'], targets['Turn_prediction'])\n",
    "        Walking_mAP = average_precision_score(targets['Walking'], targets['Walking_prediction'])\n",
    "        mAP = (Walking_mAP+Turn_mAP+StartHesitation_mAP)/3\n",
    "\n",
    "        print(f'\\n\\n[0] StartHesitation mAP - {StartHesitation_mAP:.3f}, Turn mAP - {Turn_mAP:.3f}, Walking mAP - {Walking_mAP:.3f}, mAP - {mAP:.3f}')\n",
    "        \n",
    "        scores.append(f'{mAP:.3f}')\n",
    "        \n",
    "        # Score        \n",
    "        Event_mAP = average_precision_score(targets['Event'], targets['Event_prediction'])        \n",
    "        print(f'[1] Event mAP - {Event_mAP:.3f}\\n')        \n",
    "        scores.append(f'{Event_mAP:.3f}')\n",
    "        \n",
    "        # Save\n",
    "        scores.append(f'{loss:.4f}')\n",
    "        \n",
    "        save_name = '_'.join(scores)\n",
    "        #save_path = f'./kaggle/working/{save_name}_model.h5'\n",
    "        save_path = f'/kaggle/working/{save_name}_model.h5'\n",
    "        self.model.save_weights(save_path)\n",
    "        \n",
    "'''\n",
    "Training\n",
    "        \n",
    "'''\n",
    "\n",
    "if CFG['TPU']:\n",
    "    with tpu_strategy.scope():        \n",
    "        # under tpu_strategy.scope(), FOGModel().model() will not accept provided batch size i.e., it automatically adjusts batch_size = 32/replicas.\n",
    "        #model = FOGModel().model()\n",
    "        model = FOGModel()\n",
    "        model.build(input_shape=(GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3))\n",
    "        \n",
    "        model.summary(expand_nested=True)\n",
    "        if len(WEIGHTS): model.load_weights(WEIGHTS)\n",
    "        model.compile(loss=loss_function, optimizer=tf.keras.optimizers.Adam(learning_rate=CustomSchedule(LEARNING_RATE, WARMUP_STEPS), beta_1=0.9, beta_2=0.98, epsilon=1e-9))\n",
    "        #!rm -r /kaggle/working/*\n",
    "        model.fit(dataset, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, callbacks=[PredictionFnCallback()])\n",
    "else:\n",
    "    model = FOGModel()    \n",
    "    model= model.model()\n",
    "    ##model.build(input_shape=(GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3))\n",
    "    \n",
    "    # plot graph - visual plot    \n",
    "    model.summary(expand_nested=True)\n",
    "    #tf.keras.utils.plot_model(model, to_file=\"model.jpg\", expand_nested=True, show_shapes=True)\n",
    "    \n",
    "    if len(WEIGHTS): model.load_weights(WEIGHTS)\n",
    "    model.compile(loss=loss_function, optimizer=tf.keras.optimizers.Adam(learning_rate=CustomSchedule(LEARNING_RATE, WARMUP_STEPS), beta_1=0.9, beta_2=0.98, epsilon=1e-9),run_eagerly=True) # run_eagerly=True,\n",
    "    \n",
    "    #!rm -r /kaggle/working/* # comment by me\n",
    "#     model.fit(dataset, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, callbacks=[PredictionFnCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70799b18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T14:58:58.949319Z",
     "iopub.status.busy": "2023-07-14T14:58:58.948285Z",
     "iopub.status.idle": "2023-07-14T14:58:59.164462Z",
     "shell.execute_reply": "2023-07-14T14:58:59.163366Z",
     "shell.execute_reply.started": "2023-07-14T14:58:58.949270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(16, 864, 320)]     0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (Multi  (16, 864, 320)      2463680     ['input_1[0][0]',                \n",
      " HeadAttention)                                                   'input_1[0][0]',                \n",
      "                                                                  'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (16, 864, 320)       0           ['input_1[0][0]',                \n",
      "                                                                  'multi_head_attention_10[0][0]',\n",
      "                                                                  'layer_normalization_10[0][0]', \n",
      "                                                                  'sequential_14[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (16, 864, 320)      640         ['add_12[0][0]',                 \n",
      " ormalization)                                                    'add_12[1][0]']                 \n",
      "                                                                                                  \n",
      " sequential_14 (Sequential)     (16, 864, 320)       205440      ['layer_normalization_10[0][0]'] \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| dense_24 (Dense)             (16, 864, 320)       102720      []                               |\n",
      "|                                                                                                |\n",
      "| dropout_22 (Dropout)         (16, 864, 320)       0           []                               |\n",
      "|                                                                                                |\n",
      "| dense_25 (Dense)             (16, 864, 320)       102720      []                               |\n",
      "|                                                                                                |\n",
      "| dropout_23 (Dropout)         (16, 864, 320)       0           []                               |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "==================================================================================================\n",
      "Total params: 2,669,760\n",
      "Trainable params: 2,669,760\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# this cell is just for viewing nested model summary.\n",
    "EncoderLayer().model().summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80c03516",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T14:59:04.936163Z",
     "iopub.status.busy": "2023-07-14T14:59:04.935678Z",
     "iopub.status.idle": "2023-07-14T14:59:08.591564Z",
     "shell.execute_reply": "2023-07-14T14:59:08.590367Z",
     "shell.execute_reply.started": "2023-07-14T14:59:04.936124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(16, 864, 54)]           0         \n",
      "                                                                 \n",
      " tf.math.truediv (TFOpLambda  (16, 864, 54)            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense_26 (Dense)            (16, 864, 320)            17600     \n",
      "                                                                 \n",
      " add_13 (Add)                (16, 864, 320)            0         \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (16, 864, 320)            0         \n",
      "                                                                 \n",
      " sequential_20 (Sequential)  (16, 864, 320)            13348800  \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| encoder_layer_11 (EncoderLa  (16, 864, 320)         2669760   |\n",
      "| yer)                                                          |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| multi_head_attention_11 (Mu  multiple             2463680   ||\n",
      "|| ltiHeadAttention)                                           ||\n",
      "||                                                             ||\n",
      "|| add_14 (Add)            multiple                  0         ||\n",
      "||                                                             ||\n",
      "|| layer_normalization_11 (Lay  multiple             640       ||\n",
      "|| erNormalization)                                            ||\n",
      "||                                                             ||\n",
      "|| sequential_15 (Sequential)  (16, 864, 320)        205440    ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| dense_27 (Dense)      (16, 864, 320)            102720    |||\n",
      "|||                                                           |||\n",
      "||| dropout_25 (Dropout)  (16, 864, 320)            0         |||\n",
      "|||                                                           |||\n",
      "||| dense_28 (Dense)      (16, 864, 320)            102720    |||\n",
      "|||                                                           |||\n",
      "||| dropout_26 (Dropout)  (16, 864, 320)            0         |||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| encoder_layer_12 (EncoderLa  (16, 864, 320)         2669760   |\n",
      "| yer)                                                          |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| multi_head_attention_12 (Mu  multiple             2463680   ||\n",
      "|| ltiHeadAttention)                                           ||\n",
      "||                                                             ||\n",
      "|| add_15 (Add)            multiple                  0         ||\n",
      "||                                                             ||\n",
      "|| layer_normalization_12 (Lay  multiple             640       ||\n",
      "|| erNormalization)                                            ||\n",
      "||                                                             ||\n",
      "|| sequential_16 (Sequential)  (16, 864, 320)        205440    ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| dense_29 (Dense)      (16, 864, 320)            102720    |||\n",
      "|||                                                           |||\n",
      "||| dropout_27 (Dropout)  (16, 864, 320)            0         |||\n",
      "|||                                                           |||\n",
      "||| dense_30 (Dense)      (16, 864, 320)            102720    |||\n",
      "|||                                                           |||\n",
      "||| dropout_28 (Dropout)  (16, 864, 320)            0         |||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| encoder_layer_13 (EncoderLa  (16, 864, 320)         2669760   |\n",
      "| yer)                                                          |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| multi_head_attention_13 (Mu  multiple             2463680   ||\n",
      "|| ltiHeadAttention)                                           ||\n",
      "||                                                             ||\n",
      "|| add_16 (Add)            multiple                  0         ||\n",
      "||                                                             ||\n",
      "|| layer_normalization_13 (Lay  multiple             640       ||\n",
      "|| erNormalization)                                            ||\n",
      "||                                                             ||\n",
      "|| sequential_17 (Sequential)  (16, 864, 320)        205440    ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| dense_31 (Dense)      (16, 864, 320)            102720    |||\n",
      "|||                                                           |||\n",
      "||| dropout_29 (Dropout)  (16, 864, 320)            0         |||\n",
      "|||                                                           |||\n",
      "||| dense_32 (Dense)      (16, 864, 320)            102720    |||\n",
      "|||                                                           |||\n",
      "||| dropout_30 (Dropout)  (16, 864, 320)            0         |||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| encoder_layer_14 (EncoderLa  (16, 864, 320)         2669760   |\n",
      "| yer)                                                          |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| multi_head_attention_14 (Mu  multiple             2463680   ||\n",
      "|| ltiHeadAttention)                                           ||\n",
      "||                                                             ||\n",
      "|| add_17 (Add)            multiple                  0         ||\n",
      "||                                                             ||\n",
      "|| layer_normalization_14 (Lay  multiple             640       ||\n",
      "|| erNormalization)                                            ||\n",
      "||                                                             ||\n",
      "|| sequential_18 (Sequential)  (16, 864, 320)        205440    ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| dense_33 (Dense)      (16, 864, 320)            102720    |||\n",
      "|||                                                           |||\n",
      "||| dropout_31 (Dropout)  (16, 864, 320)            0         |||\n",
      "|||                                                           |||\n",
      "||| dense_34 (Dense)      (16, 864, 320)            102720    |||\n",
      "|||                                                           |||\n",
      "||| dropout_32 (Dropout)  (16, 864, 320)            0         |||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| encoder_layer_15 (EncoderLa  (16, 864, 320)         2669760   |\n",
      "| yer)                                                          |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| multi_head_attention_15 (Mu  multiple             2463680   ||\n",
      "|| ltiHeadAttention)                                           ||\n",
      "||                                                             ||\n",
      "|| add_18 (Add)            multiple                  0         ||\n",
      "||                                                             ||\n",
      "|| layer_normalization_15 (Lay  multiple             640       ||\n",
      "|| erNormalization)                                            ||\n",
      "||                                                             ||\n",
      "|| sequential_19 (Sequential)  (16, 864, 320)        205440    ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| dense_35 (Dense)      (16, 864, 320)            102720    |||\n",
      "|||                                                           |||\n",
      "||| dropout_33 (Dropout)  (16, 864, 320)            0         |||\n",
      "|||                                                           |||\n",
      "||| dense_36 (Dense)      (16, 864, 320)            102720    |||\n",
      "|||                                                           |||\n",
      "||| dropout_34 (Dropout)  (16, 864, 320)            0         |||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " sequential_21 (Sequential)  (16, 864, 640)            4101120   \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| bidirectional_4 (Bidirectio  (16, 864, 640)         1640960   |\n",
      "| nal)                                                          |\n",
      "|                                                               |\n",
      "| bidirectional_5 (Bidirectio  (16, 864, 640)         2460160   |\n",
      "| nal)                                                          |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "=================================================================\n",
      "Total params: 17,467,520\n",
      "Trainable params: 17,467,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# this cell is just for viewing nested model summary.\n",
    "FOGEncoder().model().summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e35418",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-14T13:17:36.271647Z",
     "iopub.status.idle": "2023-07-14T13:17:36.272040Z",
     "shell.execute_reply": "2023-07-14T13:17:36.271858Z",
     "shell.execute_reply.started": "2023-07-14T13:17:36.271839Z"
    }
   },
   "outputs": [],
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5745f35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T13:28:11.734833Z",
     "iopub.status.busy": "2023-07-14T13:28:11.734416Z",
     "iopub.status.idle": "2023-07-14T13:28:11.795343Z",
     "shell.execute_reply": "2023-07-14T13:28:11.794087Z",
     "shell.execute_reply.started": "2023-07-14T13:28:11.734804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 864, 54) (16, 864, 5)\n"
     ]
    }
   ],
   "source": [
    "# for element in dataset:\n",
    "#     # element[0].shape, element[1].shape => (4, 864, 54) (4, 864, 5)\n",
    "#     model.fit(element[0],element[1], callbacks=[PredictionFnCallback()])\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af7113",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-14T13:17:36.274812Z",
     "iopub.status.idle": "2023-07-14T13:17:36.275159Z",
     "shell.execute_reply": "2023-07-14T13:17:36.274993Z",
     "shell.execute_reply.started": "2023-07-14T13:17:36.274977Z"
    }
   },
   "outputs": [],
   "source": [
    "## convert to onnx so that graph can be viewed at https://netron.app/.\n",
    "# import tf2onnx\n",
    "# import onnxruntime as rt\n",
    "\n",
    "# spec = (tf.TensorSpec((None, CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3), tf.float32, name=\"input\"),)\n",
    "# output_path = \"FOGModel.onnx\"\n",
    "\n",
    "# model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13, output_path=output_path)\n",
    "# output_names = [n.name for n in model_proto.graph.output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d0f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47c96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40f44489",
   "metadata": {
    "papermill": {
     "duration": 0.031954,
     "end_time": "2023-06-13T03:07:06.479630",
     "exception": false,
     "start_time": "2023-06-13T03:07:06.447676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Search models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3541af42",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-14T13:17:36.276274Z",
     "iopub.status.idle": "2023-07-14T13:17:36.276602Z",
     "shell.execute_reply": "2023-07-14T13:17:36.276455Z",
     "shell.execute_reply.started": "2023-07-14T13:17:36.276440Z"
    },
    "papermill": {
     "duration": 0.33069,
     "end_time": "2023-06-13T03:07:06.843066",
     "exception": false,
     "start_time": "2023-06-13T03:07:06.512376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Search for saved models in the working directory and sort them\n",
    "\n",
    "'''\n",
    "\n",
    "models = []\n",
    "for fname in os.listdir('/kaggle/working/'):\n",
    "    if 'model.h5' in fname:\n",
    "        m = {}\n",
    "        m['Path'] = '/kaggle/working/' + fname\n",
    "        for i, elem in enumerate(fname.split('_')): \n",
    "            try:\n",
    "                m[i+1] = float(elem)\n",
    "            except:\n",
    "                m[i+1] = elem\n",
    "        models.append(m)\n",
    "\n",
    "if len(models): \n",
    "    models = pd.DataFrame(models)\n",
    "    plot(models.sort_values(1)[2].values)\n",
    "    models = models.sort_values(2, ascending=False)\n",
    "    display(models.head(15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 283.724921,
   "end_time": "2023-06-13T03:07:09.700085",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-13T03:02:25.975164",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
